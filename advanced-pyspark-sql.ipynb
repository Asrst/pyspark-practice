{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\r\n",
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\r\n",
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\r\n",
      "__notebook__.ipynb  sherlock.parquet  sherlock.txt  trainsched.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wget -q 'https://assets.datacamp.com/production/repositories/3937/datasets/a367f6f461f670a364ab2a59afc25bc2e3fab157/trainsched.txt'\n",
    "!wget -q 'https://assets.datacamp.com/production/repositories/3937/datasets/213ca262bf6af12428d42842848464565f3d5504/sherlock.txt'\n",
    "!wget -q 'https://assets.datacamp.com/production/repositories/3937/datasets/de0a90e8f132c1f2846e70d7b5eec250923318d5/sherlock.parquet'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/trainsched.txt\n",
      "/kaggle/working/sherlock.parquet\n",
      "/kaggle/working/sherlock.txt\n",
      "/kaggle/working/__notebook__.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "data_paths = {}\n",
    "data_dirs = ['/kaggle/working', '/kaggle/input']\n",
    "\n",
    "# store the datapaths for all files\n",
    "for data_dir in data_dirs:\n",
    "    for dirname, _, filenames in os.walk(data_dir):\n",
    "        for filename in filenames:\n",
    "            data_paths[filename] = os.path.join(dirname, filename)\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext('local')\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f96d9bbe5c0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Print my_spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(data_paths[\"trainsched.txt\"], header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.createOrReplaceTempView('schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Inspect the columns in the table df\n",
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      14|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "query = \"select count(*) from schedule\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|     null|\n",
      "|  1|     324|San Francisco|7:59a|    8:03a|\n",
      "|  2|     324|  22nd Street|8:03a|    8:16a|\n",
      "|  3|     324|     Millbrae|8:16a|    8:24a|\n",
      "|  4|     324|    Hillsdale|8:24a|    8:31a|\n",
      "|  5|     324| Redwood City|8:31a|    8:37a|\n",
      "|  6|     324|    Palo Alto|8:37a|    9:05a|\n",
      "|  7|     324|     San Jose|9:05a|     null|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (PARTITION BY train_id ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation, step by step\n",
    "Whether to use dot notation or SQL is a personal preference. However, as demonstrated in the video exercise, there are cases where SQL is simpler. Also as demonstrated in the video lesson, there are also cases where the dot notation gives a counterintuitive result, such as when a second aggregation on a column clobbers a prior aggregation on that column. As mentioned in the video, the basic syntax of agg in pyspark is only able to do a single aggregation on each column at a time.\n",
    "\n",
    "The following exercises calculate the time of the first departure for each train line.\n",
    "\n",
    "The first two queries match. However, the second two do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|    6:06a|    6:59a|\n",
      "|     324|    7:59a|    9:05a|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|    6:59a|\n",
      "|     324|    9:05a|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()\n",
    "\n",
    "# from pyspark.sql.functions import min, max, col\n",
    "# expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "# dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "# dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time')))\n",
    "\n",
    "# df = spark.sql(\"\"\"\n",
    "# SELECT *, \n",
    "# LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \n",
    "# FROM schedule\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SQL query to obtain an identical result to dot_df\n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(unix_timestamp(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - unix_timestamp(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()\n",
    "\n",
    "# window = Window.partitionBy('train_id').orderBy('time')\n",
    "# dot_df = df.withColumn('diff_min', \n",
    "#                     (unix_timestamp(lead('time', 1).over(window),'H:m') \n",
    "#                      - unix_timestamp('time', 'H:m'))/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='The Project Gutenberg EBook of The Adventures of Sherlock Holmes')\n",
      "no.of lines: 128457\n",
      "+--------------------------------------------------------------------+\n",
      "|value                                                               |\n",
      "+--------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
      "|by Sir Arthur Conan Doyle                                           |\n",
      "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
      "|                                                                    |\n",
      "|Copyright laws are changing all over the world. Be sure to check the|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col, split, explode, length\n",
    "\n",
    "# Load the dataframe\n",
    "df = spark.read.text(data_paths['sherlock.txt'])\n",
    "\n",
    "print(df.first())\n",
    "print('no.of lines:', df.count())\n",
    "\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='the project gutenberg ebook of the adventures of sherlock holmes')\n",
      "columns: ['value']\n"
     ]
    }
   ],
   "source": [
    "df = df.select(lower(col('value')).alias('value'))\n",
    "print(df.first())\n",
    "print('columns:', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|words                                                                              |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[the, project, gutenberg, ebook, of, the, adventures, of, sherlock, holmes]        |\n",
      "|[by, sir, arthur, conan, doyle]                                                    |\n",
      "|[, #15, in, our, series, by, sir, arthur, conan, doyle, ]                          |\n",
      "|[]                                                                                 |\n",
      "|[copyright, laws, are, changing, all, over, the, world, , be, sure, to, check, the]|\n",
      "+-----------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df = df.select(split('value', '[ %s]' % punctuation).alias('words'))\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               words|length|\n",
      "+--------------------+------+\n",
      "|[the, project, gu...|    10|\n",
      "|[by, sir, arthur,...|     5|\n",
      "|[, #15, in, our, ...|    11|\n",
      "|                  []|     1|\n",
      "|[copyright, laws,...|    14|\n",
      "|[copyright, laws,...|     9|\n",
      "|[this, or, any, o...|     8|\n",
      "|                  []|     1|\n",
      "|[this, header, sh...|    12|\n",
      "|[gutenberg, file,...|    17|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"length\", size(df.words)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[the, project, gu...|\n",
      "|[by, sir, arthur,...|\n",
      "|[, #15, in, our, ...|\n",
      "|[copyright, laws,...|\n",
      "|[copyright, laws,...|\n",
      "|[this, or, any, o...|\n",
      "|[this, header, sh...|\n",
      "|[gutenberg, file,...|\n",
      "|[header, without,...|\n",
      "|[please, read, th...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "df = df.where(size(col(\"words\")) > 1)\n",
    "df.show(10)\n",
    "# user defined function \n",
    "# size_ = udf(lambda xs: len(xs), IntegerType())\n",
    "# df.where(size_(df.words) > 1).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331684\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|       the|\n",
      "|adventures|\n",
      "|        of|\n",
      "|  sherlock|\n",
      "|    holmes|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(explode('words').alias('word'))\n",
    "print(df.count())\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106370"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonblank_df = df.where(length('word') > 0)\n",
    "nonblank_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      word| id|\n",
      "+----------+---+\n",
      "|       the|  0|\n",
      "|   project|  1|\n",
      "| gutenberg|  2|\n",
      "|     ebook|  3|\n",
      "|        of|  4|\n",
      "|       the|  5|\n",
      "|adventures|  6|\n",
      "|        of|  7|\n",
      "|  sherlock|  8|\n",
      "|    holmes|  9|\n",
      "+----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = nonblank_df.select('word', monotonically_increasing_id().alias('id'))\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|word  |id |\n",
      "+------+---+\n",
      "|it    |71 |\n",
      "|do    |72 |\n",
      "|not   |73 |\n",
      "|change|74 |\n",
      "|or    |75 |\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter and show the first 5 rows\n",
    "df2.where('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+----+\n",
      "|      word| id|  title|part|\n",
      "+----------+---+-------+----+\n",
      "|       the|  0|Preface|   0|\n",
      "|   project|  1|Preface|   0|\n",
      "| gutenberg|  2|Preface|   0|\n",
      "|     ebook|  3|Preface|   0|\n",
      "|        of|  4|Preface|   0|\n",
      "|       the|  5|Preface|   0|\n",
      "|adventures|  6|Preface|   0|\n",
      "|        of|  7|Preface|   0|\n",
      "|  sherlock|  8|Preface|   0|\n",
      "|    holmes|  9|Preface|   0|\n",
      "+----------+---+-------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('title', when(df2.id < 25000,'Preface')\n",
    "                     .when(df2.id < 50000,'Chapter 1')\n",
    "                     .when(df2.id < 75000,'Chapter 2')\n",
    "                     .when(df2.id < 100000,'Chapter 3')\n",
    "                     .when(df2.id < 150000,'Chapter 4')\n",
    "                     .when(df2.id < 200000,'Chapter 5')\n",
    "                     .when(df2.id < 250000,'Chapter 6')\n",
    "                     .otherwise('Chapter 7'))\n",
    "\n",
    "df2 = df2.withColumn('part', when(df2.id < 25000, 0)\n",
    "                     .when(df2.id < 50000, 1)\n",
    "                     .when(df2.id < 75000, 2)\n",
    "                     .when(df2.id < 100000,3)\n",
    "                     .when(df2.id < 150000,4)\n",
    "                     .when(df2.id < 200000,5)\n",
    "                     .when(df2.id < 250000,6)\n",
    "                     .otherwise(7))\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.repartition(8,'part')\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part=0  part=1  part=2  part=3  part=4  part=5  part=6  part=7\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"sherlock_parts\"\n",
    "df2.write.partitionBy(\"part\").format(\"csv\").save(\"sherlock_parts\")\n",
    "!ls sherlock_parts\n",
    "\n",
    "# df_parts = spark.read.csv('sherlock_parts')\n",
    "# df_parts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-------+-------+--------+\n",
      "|part|     w1|     w2|     w3|     w4|      w5|\n",
      "+----+-------+-------+-------+-------+--------+\n",
      "|   7|   null|   null|   land|    and|     the|\n",
      "|   7|   null|   land|    and|    the|     new|\n",
      "|   7|   land|    and|    the|    new|    york|\n",
      "|   7|    and|    the|    new|   york| bankers|\n",
      "|   7|    the|    new|   york|bankers|    were|\n",
      "|   7|    new|   york|bankers|   were| sipping|\n",
      "|   7|   york|bankers|   were|sipping|   their|\n",
      "|   7|bankers|   were|sipping|  their|     tea|\n",
      "|   7|   were|sipping|  their|    tea|      in|\n",
      "|   7|sipping|  their|    tea|     in|absolute|\n",
      "+----+-------+-------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Word for each row, previous two and subsequent two words\n",
    "query = \"\"\"SELECT part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM df\n",
    "\"\"\"\n",
    "spark.sql(query).where(\"part = 7\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+----------+-----+\n",
      "|     w1|       w2|       w3|      w4|        w5|count|\n",
      "+-------+---------+---------+--------+----------+-----+\n",
      "|history|       of|      the|  united|    states|   60|\n",
      "|     in|      the|   region|      of|       the|   40|\n",
      "|     in|      the|   middle|      of|       the|   38|\n",
      "|project|gutenberg| literary| archive|foundation|   34|\n",
      "|     of|      the|   united|  states|        pp|   31|\n",
      "|    the|  project|gutenberg|literary|   archive|   30|\n",
      "|     in|      the|     case|      of|       the|   28|\n",
      "|     at|      the|      end|      of|       the|   26|\n",
      "|    the|    other|     side|      of|       the|   25|\n",
      "|     on|      the|     same|   lines|        as|   25|\n",
      "+-------+---------+---------+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 sequences of five words\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "   LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3,\n",
    "   LEAD(word, 3) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "   LEAD(word, 4) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "   FROM df\n",
    ")\n",
    "GROUP BY w1, w2, w3, w4, w5\n",
    "ORDER BY count DESC\n",
    "LIMIT 10 \"\"\"\n",
    "df5 = spark.sql(query)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---------+----------+\n",
      "|       w1|      w2|       w3|       w4|        w5|\n",
      "+---------+--------+---------+---------+----------+\n",
      "|        ~|asterisk|      and|underline|characters|\n",
      "|zygomatic|     and|  frontal|    bones|       the|\n",
      "|   zygoma|      in|    front|       of|       the|\n",
      "|    zweck|     ist|      nur|      den|     feind|\n",
      "|      zum|  henker|    diese|   russen|  muttered|\n",
      "|  zueblin|american|municipal| progress|         w|\n",
      "| zubovski| rampart|   pierre|      and|  thirteen|\n",
      "| zubovski| rampart|      but|     rose|   through|\n",
      "|   zubova|    with|    false|    curls|       and|\n",
      "|   zubova|     and|     this|     same|     laugh|\n",
      "+---------+--------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique 5-tuples sorted in descending order\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4, w5 FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "   FROM df\n",
    "   )\n",
    "   ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \n",
    "   LIMIT 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Most frequent 3-tuple per chapter\n",
    "# query = \"\"\"\n",
    "# SELECT chapter, w1, w2, w3, count FROM\n",
    "# (\n",
    "#   SELECT\n",
    "#   chapter,\n",
    "#   ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY count DESC) AS row,\n",
    "#   w1, w2, w3, count\n",
    "#   FROM ( %s )\n",
    "# )\n",
    "# WHERE row = 1\n",
    "# ORDER BY chapter ASC\n",
    "# \"\"\" % subquery\n",
    "\n",
    "# spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dataframe\n",
    "# par_df = spark.read.load(data_paths['sherlock.parquet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
